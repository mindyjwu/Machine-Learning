{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICNP_KtiX_Gf"
   },
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Homework 4: Long-Short Term Memory Networks and Ensemble of Neural Network Models\n",
    "### Due: November 17th, 2021 at 11:59PM\n",
    "\n",
    "\n",
    "### Name: Mindy Wu\n",
    "### Email: mhw370@nyu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7PutUSlX_Gi"
   },
   "source": [
    "This homework has two parts. In the first part you will implement the long-short term memory networks and train them on a specific sequence modeling task. In the second part you will build an ensemble of neural networks for a specific problem and conduct some error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1SGY3iJX_Gj"
   },
   "source": [
    "## Question P1: Long-Short Term Memory Networks (35 Points Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41WuSb6zX_Gj"
   },
   "source": [
    "In this problem you will implement a long-short term memory (LSTM) models from scratch and train them for the language modeling task. You will also do some error analysis and ablation studies on the learnt model. \n",
    "\n",
    "**At no point are you allowed to use PyTorch's nn.LSTM function in the code. Remember the goal of this assignment is to build the LSTM from scratch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxxfpQrSX_Gk"
   },
   "source": [
    "### Define some helper functions to handle the text data (do nothing here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QR-ZfWKmX_Gk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo18aXaJX_Gl"
   },
   "source": [
    "### Define some hyper-parameters of the model (do nothing here.. yet)\n",
    "We will now define some hyper-parameters to be used with the model. Later on in the assignment you will be experimenting with some of these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xudBvwp_X_Gm",
    "outputId": "51bcc90d-e69d-4156-bf78-45b39d58938d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "embed_size = 32         # size of the input feature vector representing each word\n",
    "hidden_size = 32        # number of hidden units in the LSTM cell\n",
    "num_epochs = 1          # number of epochs for which you will train your model    \n",
    "num_samples = 200       # number of words to be sampled\n",
    "batch_size = 20         # the size of your mini-batch\n",
    "seq_length = 30         # the size of the BPTT window\n",
    "learning_rate = 0.002   # learning rate of the model\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31813LfyX_Go"
   },
   "source": [
    "### Load the sequential data (do nothing here)\n",
    "\n",
    "We will use the Penn Tree Bank dataset for the purpose of this exercise. You need to download by running the command `wget https://data.deepai.org/ptbdataset.zip`, unzip it, and store it in the directory `./data/ptb`. We will only use the files `ptb.train.txt` and `ptb.test.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2At7JEwokNMq",
    "outputId": "f5d93983-04b0-4e77-bb79-3aac4b5c818d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pp5vFBdOX_Go",
    "outputId": "13e93b0c-43cc-4872-e334-52009ff59bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1549\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('./ptb.train.txt', batch_size)\n",
    "#ids = corpus.read_data('./data/ptb/ptb.train.txt', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length\n",
    "\n",
    "test_corpus = Corpus()\n",
    "test_ids = test_corpus.get_data('./ptb.test.txt', batch_size)\n",
    "test_vocab_size =  len(test_corpus.dictionary)\n",
    "test_num_batches = test_ids.size(1) // seq_length\n",
    "\n",
    "print(num_batches)\n",
    "print(test_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BMaYwx1w7MCy"
   },
   "outputs": [],
   "source": [
    "#torch.nn.LSTM??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S9Ri6ohX_Gp"
   },
   "source": [
    "### Implement the Long-Short Term Memory (LSTM) unit (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaRmqbtVkRBu",
    "outputId": "49ba1090-a6cb-4663-aca1-33f47a82bbd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMl3yt7rX_Gp"
   },
   "source": [
    "You will be implementing a single unit of the LSTM and also write its forward propagation algorithm. The single LSTM unit performs the following operations: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "i_t & = \\sigma (W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "f_t & = \\sigma (W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "g_t & = \\sigma (W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "o_t & = \\sigma (W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "h_t & = o_t \\odot \\tanh(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is called the memory cell state at time $t$, $x_t$ is the input at time $t$, $h_{t-1}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$, are the input, forget, cell, and output gates, respectively. $\\sigma$ is the sigmoid function, and $\\odot$ is the Hadamard (element-wise) product of two vectors. Finally $\\{W_{ii}, b_{ii}, W_{hi}, b_{hi}\\}$, $\\{W_{if}, b_{if}, W_{hf}, b_{hf}\\}$, $\\{W_{ig}, b_{ig}, W_{hg}, b_{hg}\\}$, and $\\{W_{io}, b_{io}, W_{ho}, b_{ho}\\}$ are the learnable weights and biases for computing the input, forget, cell, and output gates respectively.\n",
    "\n",
    "At each time step the LSTM takes as input the previous hidden state, the previous memory cell state and the embedding (features) associated with the current word and generates the new hidden states and the prediction of the next word. \n",
    "\n",
    "**Hint: Note that the recurrsion is around two variables, namely, $h_t$ and $c_t$. Hence you will need to keep track of two previous hidden states.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "4mucRCJtX_Gp",
    "outputId": "6e23eaa9-503d-461c-f9b7-fbdb7b050898"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-73cc99e58fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-73cc99e58fe8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, embed_size, hidden_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# find the weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'myLSTM' object has no attribute 'init_weights'"
     ]
    }
   ],
   "source": [
    "# myLSTM based language model\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            vocab_size: the number of unique words in your dataset \n",
    "            embed_size: the size of the feature vector assiciated with each word\n",
    "            hidden_size: the size of the hidden state features\n",
    "        \"\"\"\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) # features associated with the words in your vocabulary \n",
    "        # rest of your LSTM model code goes here\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #initialize learnable weights and biases\n",
    "        #i_t\n",
    "        self.w_ii = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
    "        self.b_ii = nn.Parameter(torch.Tensor(embed_size))\n",
    "        self.w_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_hi = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        #f_t\n",
    "        self.w_if = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
    "        self.b_if = nn.Parameter(torch.Tensor(embed_size))\n",
    "        self.w_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_hf = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        #g_t\n",
    "        self.w_ig = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
    "        self.b_ig = nn.Parameter(torch.Tensor(embed_size))\n",
    "        self.w_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_hg = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "\n",
    "        #o_t\n",
    "        self.w_io = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
    "        self.b_io = nn.Parameter(torch.Tensor(embed_size))\n",
    "        self.w_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_ho = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        def init_weights(self):\n",
    "          stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "          for weight in self.Module():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        # find the weight\n",
    "        self.init_weights()\n",
    "\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "        Perform forward propagation over a single unit composed of an LSTM unit followed by a linear layer to \n",
    "        generate the input activations for the softmax function. \n",
    "    \n",
    "        Input:\n",
    "            x: the current input (indices into the vocabulary)\n",
    "            h: the previous hidden states\n",
    "\n",
    "        Output:\n",
    "            out: the output of the unit (which are the input activations of the softmax layer to predict the next word)\n",
    "            (h, c): the current hidden states composed of the hidden state and the memory cell state\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code goes here\n",
    "        x = self.embed(x)\n",
    "        n_x, m, T_x = x.shape()\n",
    "        #batch_len, seq_len, embed_len\n",
    "        out = []\n",
    "        h_t, c_t = h\n",
    "\n",
    "\n",
    "        for t in range(m):\n",
    "            x_t = x[:, t, :]\n",
    "            gates = x_t @ self.W + h @self.U + self.bias\n",
    "\n",
    "            i_t, f_t, g_t, o_t = (\n",
    "                  torch.sigmoid(x_t @ self.w_ii + self.b_ii + h_t @ self.w_hi + self.b_hi),\n",
    "                  torch.sigmoid(x_t @ self.w_if + self.b_if + h_t @ self.w_hf + self.b_hf),\n",
    "                  torch.sigmoid(x_t @ self.w_ig + self.b_ig + h_t @ self.w_hg + self.b_hg),\n",
    "                  torch.sigmoid(x_t @ self.w_io + self.b_io + h_t @ self.w_ho + self.b_ho),\n",
    "              )\n",
    "            \n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            out.append(self.linear(o_t))\n",
    "\n",
    "\n",
    "        out = torch.cat(out, dxim=0)\n",
    "        out = out.transpose(0,1).continguous() \n",
    "        h_t = h\n",
    "        c_t = c\n",
    "        return out, (h, c)\n",
    "\n",
    "model = myLSTM(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss     # the loss function\n",
    "\n",
    "# we will use the Adam optimizer for faster and easier convergence\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGCex-m1X_Gq"
   },
   "source": [
    "### Training loop for the model (7.5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "4OhMszRqX_Gq",
    "outputId": "ba1f06f2-b4cd-4b83-9f34-e189bb636582"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3c16440197bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# we clip the gradients to ensure that they remain bounded. This is a hack!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        states = detach(states)\n",
    "        \n",
    "        ## the code for the rest of the forward pass goes here ##\n",
    "        \n",
    "        \n",
    "        # Backward pass \n",
    "        \n",
    "        ## code for backward pass goes here ##\n",
    "\n",
    "        # we clip the gradients to ensure that they remain bounded. This is a hack! \n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        # Optimize        \n",
    "        \n",
    "        ## code for optimization goes here ##\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etKrYlIyX_Gq"
   },
   "source": [
    "### Testing loop for the model (7.5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36mkgZq7X_Gr"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Test the model\n",
    "states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
    "test_loss = 0.\n",
    "with torch.no_grad():\n",
    "    for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "        targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        \n",
    "        ## code for forward pass goes here ##\n",
    "\n",
    "test_loss = test_loss / test_num_batches\n",
    "print('test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lD309hvX_Gr"
   },
   "source": [
    "### Generating text using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2_9wEniX_Gr"
   },
   "outputs": [],
   "source": [
    "# Generate texts using trained model\n",
    "with torch.no_grad():\n",
    "    with open('sample.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(1, 1, hidden_size).to(device),\n",
    "                 torch.zeros(1, 1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN \n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHkNo6pYX_Gr"
   },
   "source": [
    "**As part of your solution please submit the sample.txt file as well. Name it as \\<your-netid>_sample_hw4.txt and include it in your top level zip file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9_9kJ0oX_Gs"
   },
   "source": [
    "## Question P2: Ensemble of Neural Networks (35 Points Total)\n",
    "In this part of the assignment, you will train an ensemble of neural networks for classifying hand written digits in the MNIST dataset. You will take the code provided in the following cell and extend it to build an ensemble of single hidden layer MLP as per the specifications provided in the questions below. \n",
    "\n",
    "For the data, you need to download the MNIST dataset from ``http://yann.lecun.com/exdb/mnist/`` and place the files in the directory ``./data/MNIST``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "auIn7YhWX_Gs",
    "outputId": "3274dd12-f7fd-4804-c61f-7dde1c463edf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-15b305582c50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# initialize the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmoment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of parameters: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_n_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# train the model for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_n_params' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "\n",
    "# load the MNIST dataset\n",
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "# define the multi-layer perceptron model\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "    \n",
    "\n",
    "# define the training loop\n",
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through the model\n",
    "        output = model(data)\n",
    "        # forward pass through the cross-entropy loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # backward pass through the cross-entropy loss function and the model\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "# define the testing loop\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "    \n",
    "# build the model and execute the training and testing\n",
    "\n",
    "# initialize some hyper-paramters \n",
    "n_hidden = 8 # number of hidden units\n",
    "learning_rate = 0.01\n",
    "moment = 0.5\n",
    "nepochs = 1\n",
    "\n",
    "# build the actual model\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "#model_fnn.to(device)\n",
    "# initialize the optimizer\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=learning_rate, momentum=moment)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "# train the model for one epoch\n",
    "for epoch in range(0, nepochs):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7TsuwORX_Gs"
   },
   "source": [
    "### Question P2.a: Ensemble of networks of same size but initialized with a different seed (20 Points)\n",
    "Extend the above code to create an ensemble of `k` single hidden layer MLP models, each initialized with a different random seed. Train each model inside the ensemble for exactly `1` epoch and report the performance of the individual models and the ensemble model on the test set. For the purpose of ensembling, you can take a majority vote of the model outputs. \n",
    "\n",
    "At a high level you need to perform the following tasks:\n",
    "1. Initialize `k` models with different random seed (for some value of k): (**5 points**)\n",
    "2. Extend the train() function to train individual models inside the ensemble: (**5 points**)\n",
    "3. Extend the test() function to estimate the accuracy of the individual models and the ensemble model: (**5 points**)\n",
    "4. Repeat the above process with different values of `k`. For this exercise use $k = \\{1, 2, 4, 8, 16, 32 \\}$, and plot a graph with `k` on the x-axis and ensemble model performance on the y-axis. (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LBEKsAEOohm2"
   },
   "outputs": [],
   "source": [
    "# initialize the k models with different random seed\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ESWQMBIto0bI"
   },
   "outputs": [],
   "source": [
    "# extending the train() function to train individual models inside ensemble\n",
    "\n",
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through the model\n",
    "        output = model(data)\n",
    "        # forward pass through the cross-entropy loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # backward pass through the cross-entropy loss function and the model\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PRCcW90Wo6Do"
   },
   "outputs": [],
   "source": [
    "# extending the test() function to estimate accuracy of individual and ensemble models\n",
    "\n",
    "\n",
    "# define the testing loop\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QfOGtsDUo6GM"
   },
   "outputs": [],
   "source": [
    "# repeating the process \n",
    "\n",
    "k = {1,2,3,4,8,16,32}\n",
    "\n",
    "\n",
    "# plot the graph with k (x-axis) and ensemble model performance (y-axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzIH7WTTX_Gs"
   },
   "source": [
    "### Question P2.b: Ensemble of networks of different size but initialized with the same seed (15 Points)\n",
    "Extend the above code to create an ensemble of `k` single hidden layer MLP models, each with a different number of hidden units. In particular, use the following hidden layer values $nhid = \\{2, 4, 8, 16, 32, 64, 128\\}$. Thus here the value of `k` is $7$. Each model is initialized using the same random seed. Train each model inside the ensemble for exactly `1` epoch and report the performance of the individual models and the ensemble model on the test set. For the purpose of ensembling, you can take a majority vote of the model outputs. \n",
    "\n",
    "At a high level you need to perform the following tasks:\n",
    "1. Initialize `k=7` models with different number of hidden layers: (**5 points**)\n",
    "2. Use the same extension to the train() function as in P2.a to train individual models of the ensemble: (**2.5 points**)\n",
    "3. Use the same extension to the test() function as in P2.a to estimate the accuracy of the individual models and the ensemble model: (**2.5 points**)\n",
    "4. Plot a graph with `nhid` on the x-axis and ensemble model performance on the y-axis. (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hK18f9YYpHNb"
   },
   "outputs": [],
   "source": [
    "# initializing the k =7 models with different number of hidden layers\n",
    "\n",
    "nhid = {2,4,8,16,32,64,128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8lFOTVcdpLVu"
   },
   "outputs": [],
   "source": [
    "# Use the same extension to the train() function as in P2.a to train individual models of the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "s9L_skVwpLeW"
   },
   "outputs": [],
   "source": [
    "# Use the same extension to the test() function as in P2.a to estimate the accuracy of the individual models and the ensemble model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "sQMUtfZt23Nh"
   },
   "outputs": [],
   "source": [
    "# Plot a graph with nhid on the x-axis and ensemble model performance on the y-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QieY48u25LF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MindyWu_IntroToML_Fall2021_HW4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "374px",
    "left": "1310px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
