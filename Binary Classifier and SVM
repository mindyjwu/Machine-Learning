# This is a class homework from Machine Learning on Binary Classifier and Supported Vector Machines

# Load the packages
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn import model_selection
from sklearn.datasets import make_blobs, load_breast_cancer
from sklearn.linear_model import LogisticRegression
import pandas as pd

# General helper method to convert sci-kit datasets to Pandas DataFrame.
def sklearn_to_df(sklearn_dataset):
    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)
    df['target'] = pd.Series(sklearn_dataset.target)
    return df

cancer_dataset = load_breast_cancer() # Load the data and convert to a pandas dataframe
df = sklearn_to_df(cancer_dataset)

# Split the data.  DO NOT TOUCH THE TEST DATA FROM HERE ON!!
train_data, val_data = model_selection.train_test_split(df, test_size = 0.2) # 0.2 is 20% validation data.

# Split the features from the class labels.
X_train = train_data.drop('target', axis = 1) # We drop the target from the features.  
X_val  = val_data.drop('target', axis = 1)  # Note that this does not operate inplace.
 
y_train = train_data['target']
y_val  = val_data['target']


# Now fit a logistic regression model.
model = LogisticRegression(solver = 'liblinear', class_weight = 'balanced')
model.fit(X_train, y_train);

# get the default predictions on the validation set 
val_pred = model.predict(X_val)

# Precision score.
precision = metrics.precision_score(y_val, val_pred)
print("Precision = {:0.1f}%".format(100 * precision))

# Recall score.
recall = metrics.recall_score(y_val, val_pred)
print("Recall    = {:0.1f}%".format(100 * recall))

# get the values of the probabilities generated by the logistic regression
val_prob = model.predict_proba(X_val)


## Your code to compute and plot ROC goes here 

# since all the input: val_pred, y_val are <class 'numpy.ndarray'>
# we are trying to convert numpy array and series into a list
def toList(inputs):
    if type (inputs)== list:
        return inputs
    else:
        inputs = inputs.tolist()
    
    return inputs


# since it kept running error 
# "float division by zero" when I plot the ROC, sometimes its the length doesn't match up 
#def checkPoint(predict, valid):
#    if len(predict) == len(valid):
 #       return True
 #   else:
  #      return "length of predict and valid doesn't matchup"


# true positive
def TP(predict, valid):
    
    length = len(valid)
    tp = 0
    
    for i in range(length):
        #when it is true in valid
        if valid[i] == 1 and valid[i] == predict[i]:
            tp += 1
    return tp/length
            
    
# just to test: 
#TP(toList(val_pred), toList(y_val))
        

# false positive
def FP(predict, valid):
    
    length = len(valid)
    fp = 0
    
    for i in range(length):
        #when it is true in valid
        if valid[i] == 0 and valid[i] != predict[i]:
            fp += 1
    return fp/length
            

# true negative
def TN(predict, valid):
    
    length = len(valid)
    tn = 0
    
    for i in range(length):
        if valid[i] == 0 and valid[i] == predict[i]:
            tn += 1
    return tn/length


# false negative
def FN(predict, valid):
    
    length = len(valid)
    fn = 0
    
    for i in range(length):
        if valid[i] == 1 and valid[i] != predict[i]:
            fn += 1
    return fn/length



 # create  TPR & FPR 
def TPR (predict, valid):
    predict = toList(predict)
    valid = toList(valid)
    tpr = TP(predict, valid)/(TP(predict, valid) + FN(predict, valid))
    return float(tpr)

def FPR (predict, valid):
    predict = toList(predict)
    valid = toList(valid)
    fpr = FP(predict, valid)/(FP(predict, valid) + TN(predict, valid))
    print(fpr)
    return float(fpr)
    
# test
#print(TPR(val_pred, y_val))
#print(FPR(val_pred, y_val))

#  plot at all classification threshold by generating one new dictionary

# create a threshold
threshold = np.arange(0.0, 1, 0.01).tolist()

# check the classification of its probability being class 1
prob_class1 = model.predict_proba(X_val)[:,1]

#I discuss and reference the dictionary part of graphing this threshold from classmate, credits to Saahil and Fernando
dictionary = dict()

for i in range(len(threshold)):
    if threshold[i] not in dictionary.keys():
        new_threshold = threshold[i] #store the new threshold as a new key
        # create a new classification list for the threhold
        threshold_list = []
        
    for j in range(len(prob_class1)):
        if prob_class1[j] >= threshold[i]:
            # if the threhold for classification 1 is greater than the threshold, we append the classification as 1 
            threshold_list.append(1)
        else:
            threshold_list.append(0)
            
    #update on the dictionary
    dictionary.update({new_threshold: threshold_list})
    
    del threshold_list
    
 # make the new updated dictionary into a dataframe   
df = pd.DataFrame(dictionary)





# plots TPR (y-axis) vs. FPR (x-axis) 

tpr_threshold = []
fpr_threshold = []

for i in df.columns:
    #calculate for all threshold rates 
    tpr_threshold.append(TPR(df[i], y_val))
    fpr_threshold.append(FPR(df[i], y_val))



# plot the ROC curve

plt.plot(fpr_threshold, tpr_threshold)
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC curve")
plt.show()

## Your code to compute the AUC goes here

auc = 0
for i in range(len(threshold)-1):
    auc += (fpr_threshold[i] - fpr_threshold[i + 1]) * tpr_threshold[i]
    
print("Area Under the ROC Curve = ", auc)






# underneath is code for creating supported vector machines (SVM)


import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from scipy.optimize import minimize
import matplotlib.pyplot as plt


# Load in the data.
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Ignore the first class and use only the first 2 features.
X = X[y != 0, :2]
y = y[y != 0]

# Make sure that the class labels are either +1 or -1.
y[y==2] = -1

n_sample = len(X)

# Randomly order the data.
np.random.seed(0)
order = np.random.permutation(n_sample)
X = X[order]
y = y[order].astype(float)

# Split the data into 10% testing and 90% training.
X_train = X[:int(.9 * n_sample)]
y_train = y[:int(.9 * n_sample)]
X_test = X[int(.9 * n_sample):]
y_test = y[int(.9 * n_sample):]



def inner_product_matrix(X):
    """
    Compute the inner product matrix of the training data X where each row is a different data point x_i.
    
    Input:
        X: np.ndarray(n, p), n data points in dimension of p
    
    Return:
        M: np.ndarray(n, n), each entry is the inner product of the corresponding pair of vectors m_{ij}
    """
    ##TODO-start##
    M = np.dot(X, X.T)
    return M
    ##TODO-end##
    
print(inner_product_matrix(X_test))



def objective_function(a, X, y):
    """
    The objective function of the dual problem W.
    
    Input:
        a: np.ndarray(n,), the parameter alpha we want to optimize
        X: np.ndarray(n, p), the matrix of training data features
        y: np.ndarray (n,), the vector of training data labels, must be either +1 or -1.
    
    Return:
        W: float, value of the objective function. 
    """
    
    ##TODO-start## 
    
    W =  np.sum(a) - .5 * np.sum((y[None, :] * y[:, None]) * (a[None, :] * a[:, None]) * inner_product_matrix(X))
    ##TODO-end##
    return W
    
    
    
    
    # objective_function(a, X, y)
objective_function(np.random.rand(len(y_train)), X_train, y_train)




def fit(X, y, C):
    """
    Computes the parameters alpha and bias that determine the maximum-margin decision boundary for SVM.
    
    Input:
        X: np.ndarray(n,p), matrix of training data features
        y: np.ndarray(n, ), vector of training data labels
        C: float, slack parameter that is non-negative
        
    Return:
        w: np.ndarray(p,), vector of primal variable values (vector orthogonal to decision boundary)
        bias: float, the bias term in SVM
        alpha: np.ndarray(n, ), vector of dual variable values
        
        
Get the box constraints  0â‰¤ð›¼ð‘–â‰¤ð¶  for  ð‘–=1,â€¦,ð‘› . You will pass this into minimize() as the bounds argument.

Get the linear constraint  âˆ‘ð‘›ð‘–=1ð›¼ð‘–ð‘¦ð‘–=0 . You will pass this into minimize() as the constraints argument.

Call minimize() using the correct objective function  âˆ’ð‘Š(ð›¼)  as well as the constraints and bounds from the previous 2 parts. Use  ð›¼0=0âˆˆâ„ð‘›  as the initial point and use the SLSQP method.


    """
    
    ## TO-DO STARTS HERE##
    #parameter of alpha we try to optimizaed
    a = np.random.rand(len(X)) # from the objective function
    
    fun = lambda a: -objective_function(a,X,y)
    x0 = np.random.rand(len(y))
    # create the bounds for the input space
    bounds = 90 * [(0,C)]
    constrains = ({"type" : "eq", "fun" : lambda a: np.dot(a, y)}) 

    res = minimize(fun = fun, x0 = x0, bounds = bounds, constraints = constrains, method = "SLSQP")
    alpha = res.x
    
       
    # Compute the primal variables w.
    w = np.sum((alpha[:, None] * X * y[:, None]), axis=0)  
    
    # Compute the bias.
    bias = 1/len(y) * np.sum(y[:, None] - w.T * X)
    
    
    ## TO-DO ENDS HERE ##
    return (w, alpha, bias)
    
    
    
    w, alpha, bias = fit(X_train, y_train, C = 10)
    
    
    plt.figure(1)
plt.clf()
plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,
            edgecolor='k', s=20)

# Circle out the test data
plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',
            zorder=10, edgecolor='k')

plt.axis('tight')
x_min = X[:, 0].min()
x_max = X[:, 0].max()
y_min = X[:, 1].min()
y_max = X[:, 1].max()

XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
XXYY = np.c_[XX.ravel(), YY.ravel()]
ZZ = []
for i in range(XXYY.shape[0]):
    ZZ.append(XXYY[i]@w+bias)
    
Z = np.array(ZZ)

# Put the result into a color plot
Z = Z.reshape(XX.shape)
plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],
            linestyles=['--', '-', '--'], levels=[-.5, 0, .5])
plt.title('Linear SVM')
plt.xlabel(r'$x_1$')
plt.ylabel(r'$x_2$')
plt.show()




def predict(x_test, w, bias):
    """
    Compute the predictions y_pred on the test set using only the support vectors.
    
    Input:
        x_test: np.ndarray(n,p), matrix of the test data
        alpha: np.ndarray(n,), vector of the dual variables
        bias: float, the bias term
    
    Output:
        y_pred: np.ndarray(n,), vector of the predicted labels, either +1 or -1
    """
    ##TODO-start##
    y_pred = []
    size = len(x_test)
    
    
    for i in range(size):
        pred = np.sign(np.dot(w, x_test[i]) + bias)
        y_pred.append(pred)
    return y_pred
    ##TODO-end##


    
def accuracy(y_pred, y_true):
    """
    Computes the accuracy on the test set given the class predictions.
    
    Input:
        y_pred: np.ndarray(n,), vector of predicted class labels
        y_true: np.ndarray(n,), vector of true class labels
    
    Output:
        float, accuracy of predictions
    """
    return np.mean(y_pred*y_true > 0)

y_pred = predict(X_test, w, bias)
y_pred_train = predict(X_train, w, bias)
print("Training accuracy = {:0.2f}%".format(100*accuracy(y_pred_train, y_train)))
print("Testing accuracy = {:0.2f}%".format(100*accuracy(y_pred, y_test)))
